{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import random as rn\n",
    "random.seed(12345)\n",
    "import tensorflow as tf\n",
    "np.random.seed(12345)\n",
    "rn.seed(12345)\n",
    "from underthesea import word_tokenize,  pos_tag\n",
    "import os, pickle, re, keras, sklearn, string\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from scipy import spatial\n",
    "import string\n",
    "with open(\"sorted.json\",\"rb\") as h:\n",
    "    read_data=json.load(h)\n",
    "with open(\"set_addressdict_official.pickle\",\"rb\") as h:\n",
    "    address_pickle_set=pickle.load(h) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalText(sent):\n",
    "    sent = str(sent).replace('_',' ').replace('/',' trên ')\n",
    "    sent = re.sub('-{2,}','',sent)\n",
    "    sent = re.sub('\\\\s+',' ', sent)\n",
    "    patPrice = r'([0-9]+k?(\\s?-\\s?)[0-9]+\\s?(k|K))|([0-9]+(.|,)?[0-9]+\\s?(triệu|ngàn|trăm|k|K|))|([0-9]+(.[0-9]+)?Ä‘)|([0-9]+k)'\n",
    "    patHagTag = r'#\\s?[aăâbcdđeêghiklmnoôơpqrstuưvxyàằầbcdđèềghìklmnòồờpqrstùừvxỳáắấbcdđéếghíklmnóốớpqrstúứvxýảẳẩbcdđẻểghỉklmnỏổởpqrstủửvxỷạặậbcdđẹệghịklmnọộợpqrstụựvxỵãẵẫbcdđẽễghĩklmnõỗỡpqrstũữvxỹAĂÂBCDĐEÊGHIKLMNOÔƠPQRSTUƯVXYÀẰẦBCDĐÈỀGHÌKLMNÒỒỜPQRSTÙỪVXỲÁẮẤBCDĐÉẾGHÍKLMNÓỐỚPQRSTÚỨVXÝẠẶẬBCDĐẸỆGHỊKLMNỌỘỢPQRSTỤỰVXỴẢẲẨBCDĐẺỂGHỈKLMNỎỔỞPQRSTỦỬVXỶÃẴẪBCDĐẼỄGHĨKLMNÕỖỠPQRSTŨỮVXỸ]+'\n",
    "    patURL = r\"(?:http://|www.)[^\\\"]+\"\n",
    "    sent = re.sub(patURL,'website',sent)\n",
    "    sent = re.sub(patHagTag,' hagtag ',sent)\n",
    "    sent = re.sub(patPrice, ' giá_tiền ', sent)\n",
    "    sent = re.sub('\\.+','.',sent)\n",
    "    sent = re.sub('(hagtag\\\\s+)+',' hagtag ',sent)\n",
    "    sent = re.sub('\\\\s+',' ',sent)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def normalize_elonge_word(sent):\n",
    "    s_new = ''\n",
    "    for word in sent.split(' '):\n",
    "        word_new = ''\n",
    "        for char in word.strip():\n",
    "            if char != word_new[-1]:\n",
    "                word_new += char\n",
    "    s_new += word_new.strip() + ' '\n",
    "    return s_new\n",
    "\n",
    "def tokenizer(text):\n",
    "      token = word_tokenize(text,format=\"text\")\n",
    "      token = token.replace('giá tiền','giá_tiền').replace('Giá tiền','Giá_tiền')\n",
    "      return token\n",
    "\n",
    "def deleteIcon(text):\n",
    "    text = text.lower()\n",
    "    s = ''\n",
    "    pattern = r\"[a-zA-ZaăâbcdđeêghiklmnoôơpqrstuưvxyàằầbcdđèềghìklmnòồờpqrstùừvxỳáắấbcdđéếghíklmnóốớpqrstúứvxýảẳẩbcdđẻểghỉklmnỏổởpqrstủửvxỷạặậbcdđẹệghịklmnọộợpqrstụựvxỵãẵẫbcdđẽễghĩklmnõỗỡpqrstũữvxỹAĂÂBCDĐEÊGHIKLMNOÔƠPQRSTUƯVXYÀẰẦBCDĐÈỀGHÌKLMNÒỒỜPQRSTÙỪVXỲÁẮẤBCDĐÉẾGHÍKLMNÓỐỚPQRSTÚỨVXÝẠẶẬBCDĐẸỆGHỊKLMNỌỘỢPQRSTỤỰVXỴẢẲẨBCDĐẺỂGHỈKLMNỎỔỞPQRSTỦỬVXỶÃẴẪBCDĐẼỄGHĨKLMNÕỖỠPQRSTŨỮVXỸ,._]\"\n",
    "    \n",
    "    for char in text:\n",
    "        if char !=' ':\n",
    "            if len(re.findall(pattern, char)) != 0:\n",
    "                s+=char\n",
    "            elif char == '_':\n",
    "                s+=char\n",
    "        else:\n",
    "            s+=char\n",
    "    s = re.sub('\\\\s+',' ',s)\n",
    "    return s.strip()\n",
    "\n",
    "def normalize_elonge_word(sent):\n",
    "    s_new = ''\n",
    "    for word in sent.split(' '):\n",
    "        word_new = ' '\n",
    "        for char in word.strip():\n",
    "            if char != word_new[-1]:\n",
    "                word_new += char\n",
    "        s_new += word_new.strip() + ' '\n",
    "    return s_new.strip()  \n",
    "\n",
    "def clean_doc(doc,set_work=False):\n",
    "    for punc in string.punctuation:\n",
    "        doc = doc.replace(punc,' '+ punc + ' ')\n",
    "    doc = normalText(doc)\n",
    "    doc = deleteIcon(doc)\n",
    "    if set_work:\n",
    "      doc = word_tokenize(doc,format=\"text\")\n",
    "    # Lowercase\n",
    "    doc = doc.lower()\n",
    "    # Removing multiple whitespaces\n",
    "    doc = re.sub(r\"\\?\", \" \\? \", doc)\n",
    "    # Remove numbers\n",
    "    doc = re.sub(r\"[0-9]+\", \" num \", doc)\n",
    "    # Split in tokens\n",
    "    # Remove punctuation\n",
    "    for punc in string.punctuation:\n",
    "      if punc not in \"_\":\n",
    "          doc = doc.replace(punc,' ')\n",
    "    doc = re.sub('\\\\s+',' ',doc)\n",
    "    doc = normalize_elonge_word(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gist.github.com/J2TEAM/9992744f15187ba51d46aecab21fd469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_elemts(h_dict):\n",
    "    return h_dict[2]+\" \"+h_dict[1]\n",
    "address=[]\n",
    "for province in read_data:\n",
    "    text_province=extract_elemts(province)\n",
    "    for ward in province[4]:\n",
    "        text_ward=extract_elemts(ward)\n",
    "        for thi_tran in ward[4]:\n",
    "            text_thi_tran=extract_elemts(thi_tran)\n",
    "            full_addresses=f'{text_thi_tran}, {text_ward},{text_province}'\n",
    "            address.append(full_addresses)\n",
    "address_from_Foody=[]\n",
    "for h in address_pickle_set:\n",
    "    for k in list(address_pickle_set[h]):\n",
    "        address.append(k.replace(\", \\n\",\"\").replace(\"\\n\",\"\"))\n",
    "address_=[clean_doc(text,True) for text in address]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternatives={'phường':['ph','p','phuong'],\n",
    "'quận':['quan','q'],\n",
    "'thành_phố':['thanh_pho','tp','t p'],\n",
    "'tp':['thanh_pho','tp','t p'],\n",
    "'thị_xã':['thi_xa','tx','t x'],\n",
    "'thị_trấn':['tt','t','thi_tran'],\n",
    "'tt':['thị_trấn','t','thi_tran'],\n",
    "'p':['phường','phuong'],\n",
    "'xã':['xa','x'],\n",
    "'cao_đẳng':['cao_dang','cd','c d'],\n",
    "'câu_lạc_bộ': ['cau_lac_bo','clb','c l b'],\n",
    "'công_ty cổ_phần':['cong_ty co_phan','ctcp','c t c p','cty cp','cty c p'],\n",
    "'công ty':['cong_ty','cty'],\n",
    "'công_viên văn_hóa':['cong_vien van_hoa','cvvh','c v v h'],\n",
    "'công_viên':['cong_vien','cv','c v'],\n",
    "'đại_học':['dai_hoc','dh','d h'],\n",
    "'đường sắt':['duong sat','ds','d s'],\n",
    "'khu_công_nghiệp':['khu_cong_nghiep','kcn','k c n'],\n",
    "'khu du_lịch':['khu du_lich','kdl','k d l'],\n",
    "'khu nghỉ_mát':['khu nghi_mat','knm','k n m'],\n",
    "'khách_sạn':['khach_san','ks','k / s'],\n",
    "'nhà_hát': ['nha hat','nh','n h'],\n",
    "'nhà thi_đấu':['nha thi_dau','ntd','n t d'],\n",
    "'nhà_thờ':['nha tho','nt','n t'],\n",
    "'rạp hát':['rap hat','rh','r h'],\n",
    "'sân_bay quốc_tế':['san_bay quoc_te','sbqt','s b q t'],\n",
    "'sân_bay':['san bay','sb','s b'],\n",
    "'sân vận_động':['san van dong','svd','s v d'],\n",
    "'tiểu_học':['tieu_hoc','th','t h'],\n",
    "\"trung_học_cơ_sở\":['trung_hoc_co_so','thcs','t h c s'],\n",
    "\"trung_học_phổ_thông\":['trung_hoc_pho_thong','thpt','t h p t'],\n",
    "\"trung_tâm thương_mại\":['trung_tam thuong_mai','tttm','t t t m'],\n",
    "\"mall\":[\"trung_tâm thương_mại\",'trung_tam thuong_mai','tttm','t t t m'],\n",
    "'trung_tâm':[\"trung_tam\",'tt','t t'],\n",
    "'tổng công_ty':['tong cong_ty','tcty','tct'],\n",
    "'viện_bảo_tàng':['vien_bao_tang','vbt','v b t'],\n",
    "'vươn quốc_gia':['vuon quoc_gia','vqg','v q g'],\n",
    "'công_trường':['cong_truong','ct','c t'],\n",
    "'đại_lộ':['dai_lo','đl','d l'],\n",
    "'đường':['duong','d'],\n",
    "'đường hẻm':['duong hem','hẻm'],\n",
    "'đường nhỏ':['duong nho'],\n",
    "'đường_phố':['duong pho'],\n",
    "'quảng_trường':['quang truong','qt','q t'],\n",
    "'quốc_lộ':['quoc lo','ql','q l'],\n",
    "'tỉnh lộ':['tinh lo','tl','t l'],\n",
    "'Hotel':['khach_san','Khách_san','Khách_sạn']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'tổng công_ty'"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "word_tokenize(\"tổng công ty\",format=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'sub_text'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-6a9b94adc310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msub_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_split\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msub_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malternatives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchange_phrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malternatives\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sub_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0mnew_augment_Text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchange_phrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_augment_Text\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sub_text'"
     ]
    }
   ],
   "source": [
    "##removing accents\n",
    "s1 = u'ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝàáâãèéêìíòóôõùúýĂăĐđĨĩŨũƠơƯưẠạẢảẤấẦầẨẩẪẫẬậẮắẰằẲẳẴẵẶặẸẹẺẻẼẽẾếỀềỂểỄễỆệỈỉỊịỌọỎỏỐốỒồỔổỖỗỘộỚớỜờỞởỠỡỢợỤụỦủỨứỪừỬửỮữỰựỲỳỴỵỶỷỸỹ'\n",
    "s0 = u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYy'\n",
    "def remove_accents(input_str):\n",
    "\ts = ''\n",
    "\tfor c in input_str:\n",
    "\t\tif c in s1:\n",
    "\t\t\ts += s0[s1.index(c)]\n",
    "\t\telse:\n",
    "\t\t\ts += c\n",
    "\treturn s\n",
    "\n",
    "from tqdm import tqdm\n",
    "for index,text in tqdm(enumerate(address_)):\n",
    "    text_split=text.split(\" \")\n",
    "    for sub_text in text_split:\n",
    "        if sub_text in alternatives.keys():\n",
    "            for change_phrase in list(alternatives[sub_text]):\n",
    "                new_augment_Text=text.replace(sub_text,change_phrase)\n",
    "                print(new_augment_Text+\"\\n\")\n",
    "                address_.append(new_augment_Text)\n",
    "                address_.append(remove_accents(new_augment_Text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}